{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG Confidence Regression\n",
        "Train lightweight regressors to predict recall@k from retrieval-side features (top-10 and top-100)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup & imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '.venv (Python 3.11.2)' requires the ipykernel package.\n",
            "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Users/Mateusz/Advanced/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import metrics\n",
        "from scipy.stats import pearsonr\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "BASE = Path('results')\n",
        "DATASETS = {\n",
        "    'top10': BASE / 'rag_confidence_training_10.csv',\n",
        "    'top100': BASE / 'rag_confidence_training_100.csv',\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dfs = {}\n",
        "for name, path in DATASETS.items():\n",
        "    df = pd.read_csv(path)\n",
        "    dfs[name] = df\n",
        "    target_col = [c for c in df.columns if c.startswith('recall_at_')][0]\n",
        "    print(f\"{name}: {path} -> {df.shape[0]} rows, {df.shape[1]} cols, target={target_col}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Target distribution diagnostics\n",
        "Histograms of recall@k for each dataset help highlight whether the label range is saturated (e.g., top-100).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, len(dfs), figsize=(5 * len(dfs), 4), sharey=True)\n",
        "if len(dfs) == 1:\n",
        "    axes = [axes]\n",
        "for ax, (name, df) in zip(axes, dfs.items()):\n",
        "    target_col = [c for c in df.columns if c.startswith('recall_at_')][0]\n",
        "    sns.histplot(df[target_col], bins=20, kde=True, ax=ax)\n",
        "    ax.set_title(f\"{name}: {target_col}\")\n",
        "    ax.set_xlabel('Recall value')\n",
        "    ax.set_ylabel('Count')\n",
        "plt.suptitle('Label distribution per dataset', y=1.02)\n",
        "plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature-target correlations\n",
        "Correlations show which regressors drive the target and whether any feature provides signal.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(len(dfs), 1, figsize=(7, 3 * len(dfs)))\n",
        "if len(dfs) == 1:\n",
        "    axes = [axes]\n",
        "for ax, (name, df) in zip(axes, dfs.items()):\n",
        "    target_col = [c for c in df.columns if c.startswith('recall_at_')][0]\n",
        "    feature_cols = [c for c in df.columns if c not in {'query', 'query_id', target_col}]\n",
        "    corr = df[feature_cols + [target_col]].corr()[target_col].drop(target_col)\n",
        "    corr = corr.sort_values(ascending=False)\n",
        "    sns.barplot(x=corr.values, y=corr.index, ax=ax, palette='viridis')\n",
        "    ax.set_title(f\"{name}: feature correlations vs {target_col}\")\n",
        "    ax.set_xlabel('Pearson correlation')\n",
        "plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helpers: feature selection and training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_xy(df: pd.DataFrame):\n",
        "    target_col = [c for c in df.columns if c.startswith('recall_at_')][0]\n",
        "    drop_cols = {'query', 'query_id', target_col}\n",
        "    feature_cols = [c for c in df.columns if c not in drop_cols]\n",
        "    X = df[feature_cols]\n",
        "    y = df[target_col]\n",
        "    return X, y, feature_cols, target_col\n",
        "\n",
        "def train_and_eval(df: pd.DataFrame, alphas=(0.01, 0.1, 1.0, 10.0, 100.0)):\n",
        "    X, y, feature_cols, target_col = prepare_xy(df)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=RANDOM_STATE\n",
        "    )\n",
        "\n",
        "    model = Pipeline(\n",
        "        steps=[\n",
        "            ('scaler', StandardScaler()),\n",
        "            ('reg', RidgeCV(alphas=alphas, cv=5)),\n",
        "        ]\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_test)\n",
        "\n",
        "    mae = metrics.mean_absolute_error(y_test, preds)\n",
        "    mse = metrics.mean_squared_error(y_test, preds)\n",
        "    r2 = metrics.r2_score(y_test, preds)\n",
        "    corr, _ = pearsonr(y_test, preds)\n",
        "\n",
        "    coefs = model.named_steps['reg'].coef_\n",
        "    coef_table = pd.DataFrame({\n",
        "        'feature': feature_cols,\n",
        "        'coef': coefs,\n",
        "    }).sort_values('coef', ascending=False)\n",
        "\n",
        "    metrics_dict = {\n",
        "        'mae': mae,\n",
        "        'mse': mse,\n",
        "        'rmse': np.sqrt(mse),\n",
        "        'r2': r2,\n",
        "        'pearson': corr,\n",
        "        'best_alpha': float(model.named_steps['reg'].alpha_),\n",
        "    }\n",
        "\n",
        "    return model, metrics_dict, coef_table\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train & evaluate (top-10 vs top-100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = {}\n",
        "for name, df in dfs.items():\n",
        "    model, metrics_dict, coef_table = train_and_eval(df)\n",
        "    results[name] = {'metrics': metrics_dict, 'coefs': coef_table}\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    for k, v in metrics_dict.items():\n",
        "        print(f\"{k}: {v:.4f}\")\n",
        "    display(coef_table.head(10))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}